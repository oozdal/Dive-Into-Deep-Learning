{
 "cells": [
  {
   "cell_type": "raw",
   "id": "08bcdffc-b968-4615-b5b4-26cfd1ab709b",
   "metadata": {},
   "source": [
    "1) A magician has two cards: one is white on both faces and the other is black on one side and white on the other. The magician picks one card at random (with probability 1/2 for each) and throws it on the floor in a way that does not favour a card side. The card on the floor is white on its visible side. What is the probability that the other side is white as well?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e22b2344-33ab-4104-80d1-b7998b65e09e",
   "metadata": {},
   "source": [
    "Exactly one of the two cards has two white sies, so P (G1 , G2) = 1/2\n",
    "Exactly three of the four sides that could be seen initially are white P(G1) = 3/4\n",
    "Thus, P(G2 | G1) = (1/2)/(3/4) = 2/3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcfc79b2-b58b-4a42-8b87-dfad70973ed6",
   "metadata": {},
   "source": [
    "2) Explain the machine learning modelling lifecycle. Walk us through its steps and subcomponents."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e10ec9bc-a147-4cbc-8b6a-53efcc81be41",
   "metadata": {},
   "source": [
    "The machine learning life cycle is the cyclical process that data science projects follow. It defines each step that an organization should follow to take advantage of machine learning and artificial intelligence (AI) to derive practical business value.\n",
    "\n",
    "* Define Project Objectives\n",
    "\n",
    "* Acquire and Data Exploration\n",
    "\n",
    "* Modelling\n",
    "\n",
    "* Interpretion and Communicate\n",
    "\n",
    "* Implementation, Document, and Maintain"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b4e6b5f-ea8a-4238-a1cd-d47b92be7805",
   "metadata": {},
   "source": [
    "3) Explain the No Free Lunch theorem and what it means for applied machine learning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1317a34-fc3a-49a5-9af6-8d7d870f51a0",
   "metadata": {},
   "source": [
    "The No Free Lunch Theorem is implies that there is no single best optimization algorithm. The theorem states that all optimization algorithms perform equally well when their performance is averaged across all possible problems. It also implies that there is no single best machine learning algorithm for predictive modeling problems such as classification and regression.\n",
    "\n",
    "Given that there is no best single machine learning algorithm across all possible prediction problems, it motivates the need to continue to develop new learning algorithms and to better understand algorithms that have already been developed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9984ecde-f291-4a31-a3e4-bcc174f9dbb9",
   "metadata": {},
   "source": [
    "4) What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0047a43-3900-45d5-ae86-91f89fce2a06",
   "metadata": {},
   "source": [
    "In Gradient Descent (GD), we perform the forward pass using ALL the train data before starting the backpropagation pass to adjust the weights. This is called (one epoch). In Stochastic Gradient Descent (SGD), we perform the forward pass using a SUBSET of the train set followed by backpropagation to adjust the weights.\n",
    "\n",
    "While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.\n",
    "\n",
    "Thus, if the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample.\n",
    "\n",
    "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4d21950-e636-42ee-91a0-db8c749a1903",
   "metadata": {},
   "source": [
    "5. Explain the concept of loss function."
   ]
  },
  {
   "cell_type": "raw",
   "id": "950c9c3f-d713-49e2-a6fe-d2b124eb1f49",
   "metadata": {},
   "source": [
    "The loss function is a way to measure whether the algorithm is doing a good job. This measures the distance between the algorithmâ€™s current output and its expected output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f88a1a6b-0fdb-4090-83c0-2e023d43b7a9",
   "metadata": {},
   "source": [
    "6. What is the centred-log ratio transformation applied for in geoscience and why?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "602574ae-0c27-45ea-9a87-1556173abaa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c22f443f-e9d4-402d-89a2-a0a234f4b9fa",
   "metadata": {},
   "source": [
    "7. Explain Principle Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "52acb651-1456-436a-ad76-6ab6e382862f",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCS) is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, \n",
    "by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system \n",
    "such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6814f2e-b12a-4e8a-aa07-607018a3a0de",
   "metadata": {},
   "source": [
    "8. What is the curse of dimensionality and what are 3 ways for dimensionality reduction?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af31ae20-19f8-4a0e-a7a4-b6ac07b01a5a",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces \n",
    "that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.\n",
    "\n",
    "If the error increases with the increase in the number of features, it indicates the curse of dimensionality. Algorithms are harder to design in high dimensions \n",
    "and often have a running time exponential in the dimensions.\n",
    "\n",
    "Four well-known techniques for dimensionality reduction are:\n",
    "Forward Feature Construction\n",
    "Backward Feature Elimination\n",
    "Principal Component Analysis (PCA)\n",
    "t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86896a02-dfc9-48dd-90c1-1ee0b8eb3d29",
   "metadata": {},
   "source": [
    "9. Consider a dataset consisting of different geological features for a given area, along with a 50 m x 50 m soil survey assayed for multi-elements and covering the property. We are interested into finding which characteristics influence the gold values and how to predict it. Briefly discuss the appropriateness of each of the following model families in resolving this question.\n",
    "a. Linear regression, possibly with regularization\n",
    "b. Single decision tree\n",
    "c. Random forest\n",
    "d. Boosting ensemble e. Neural network\n",
    "e. Neural network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "342018eb-8f03-4cc7-8dad-e688e2663e80",
   "metadata": {},
   "source": [
    "10. Describe input variables that you would use in a model attempting to predict gold values and which algorithm would you use and why?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47f7e275-7be7-4b79-9856-5b841d88163a",
   "metadata": {},
   "source": [
    "Since Gold prices usually exhibits negative correlation when S&P500 has an extreme negative movement, I would use S&P500 prices as input and do a time series analysis (Regression Modelling) to predict the 14-day and 22-day horizons. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "04ff6a7f-797c-4b65-8b6a-b9287744e978",
   "metadata": {},
   "source": [
    "11. Consider the following graph, representing a bivariate data set and 3 models, Red Green and Blue, for the relationship between x and y, trained on the whole displayed data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ce6e64f-6417-4971-a039-8da20a4c6321",
   "metadata": {},
   "source": [
    "a) Which model would you rather use? I would use Multiple Linear Regression and Polynomial Regression\n",
    "\n",
    "b. Which one has the highest capacity? The lowest?\n",
    "Blue has the highest capacity while the red has the lowest.\n",
    "\n",
    "c. Which one has the smallest mean squared error (MSE) on the displayed data? The\n",
    "largest MSE?\n",
    "Blue has the smallest squared error,\n",
    "Red has the largest squared error.\n",
    "\n",
    "d. Which one will likely have the smallest MSE on new data from the same process with x\n",
    "values within the [0,1] interval? The largest MSE?\n",
    "The blue will have the smallest MSE. The red will have the largest MSE. \n",
    "\n",
    "f) Identify model Red and suggest possibilities for models Green and Blue.\n",
    "The model Red is Simple Linear Regression\n",
    "The model Blue and Green can be Multiple Linear Regression or Polynomial Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56d47caf-cade-4a7a-9143-9b738ecbb0c4",
   "metadata": {},
   "source": [
    "12. How can you detect underfitting & overfitting based on train and test set performance metrics?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cde72e7c-441d-4a68-acee-8780c909d90c",
   "metadata": {},
   "source": [
    "Overfitting -> when the model's error on the training set is very low but then the model's error on the test set is large! \n",
    "Underfitting -> when the model's error on both the training and test sets is very high!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c87fe477-fe4b-4e1b-ac26-7767d0a44b95",
   "metadata": {},
   "source": [
    "13. An array contains every integer from 1 to n except one element of the suite. Describe an efficient algorithm for finding the value of the missing integer. What is the time complexity of the algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5c213dd-4a25-4164-bacc-4e2adc085d11",
   "metadata": {},
   "source": [
    "The time complexity of this solution is O(n) and it requires O(1) extra space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dc0d3a3-b8ae-46f5-bbb8-a473bbdcdef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = [5, 6, 3, 1, 2]\n",
    "\n",
    "def missing_integer():\n",
    "    for i in range(1, len(x)):\n",
    "        if i not in x:\n",
    "            return i\n",
    "\n",
    "print(missing_integer())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a2ae482-5527-4df3-88d5-bda0bac9598c",
   "metadata": {},
   "source": [
    "14. In Python numpy or R, given a matrix X of strictly positive real numbers, provide an efficient and elegant code outputting. The submatrix of X consisting of the first half (rounded up) of the columns.\n",
    "a. The matrix X with the rows in reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68fa1d9e-9bfa-453a-a0b8-476d6dd9c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix before changing column order:\n",
      "\n",
      "[5 0 7]\n",
      "[3 4 2]\n",
      "[5 7 6]\n",
      "\n",
      "Matrix after changing column order:\n",
      "\n",
      "[7 0 5]\n",
      "[2 4 3]\n",
      "[6 7 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix_1 = np.random.randint(10, size=(3, 3))\n",
    "\n",
    "# creating an empty array to store the reversed column matrix\n",
    "matrix_2 = []\n",
    " \n",
    "# looping through matrix_1 and appending matrix_2\n",
    "for i in range(len(matrix_1)):\n",
    "    matrix_2.append(matrix_1[i][::-1])\n",
    " \n",
    "print('Matrix before changing column order:\\n')\n",
    "for rows in matrix_1:\n",
    "    print(rows)\n",
    "print('\\nMatrix after changing column order:\\n')\n",
    "for rows in matrix_2:\n",
    "    print(rows)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac962d21-7b5a-453a-8b46-ec2c2e920d83",
   "metadata": {},
   "source": [
    "b. The maximum element of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5c13b1-cb74-4f89-863e-80da6d102858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(matrix_1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ae741b7-ceaf-4368-b510-22eda891fa51",
   "metadata": {},
   "source": [
    "c. The median of each column of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e45c3c-0215-4621-8242-646d9bb5c29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(matrix_1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9135603-665b-40aa-b759-dad59c9c5a7f",
   "metadata": {},
   "source": [
    "d. A normalized version of X where each row sums to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd69f33-4266-4229-ab63-ca22a2dae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41666667 0.         0.58333333]\n",
      " [0.33333333 0.44444444 0.22222222]\n",
      " [0.27777778 0.38888889 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "norm_array = preprocessing.normalize(matrix_1, norm=\"l1\")\n",
    "print(norm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fe509-5a42-4020-942d-519ae511a4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a91c9f-4cd9-474e-a76c-6d66f2a32571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
